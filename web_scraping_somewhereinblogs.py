# -*- coding: utf-8 -*-
"""web_scraping_somewhereinblogs.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1bJO1xgnndv20kpWJM8roCL7wJJIDc_Uh
"""

from bs4 import BeautifulSoup
import re
import requests
import time
import random
import numpy as np
import pandas as pd

blog_base_url = 'https://www.somewhereinblog.net/'

base_url = "https://www.somewhereinblog.net/live/"
page_no = 15

def get_page_url(no):
  """
  It takes a page number as an argument, and returns a tuple of the HTTP status code and the HTML of
  the page
  
  :param no: the page number
  :return: success and get_html
  """
  page_url = base_url + str(no)
  # print(page_url)
  success = requests.get(page_url).status_code
  get_html = requests.get(page_url).text
  return success, get_html

success_code, page_html = get_page_url(page_no)

def get_stories_link(page):
  """
  It takes the HTML of a page as an argument, and returns a list of links to the stories on that page.
  
  :param page: the html of the page we're scraping
  """
  soup = BeautifulSoup(page_html, 'lxml')
  stories_link_div = soup.find_all('div', class_= 'blog-content')
  for div in stories_link_div:
      stories_link.append(blog_base_url + div.a.get('href'))

# success_code, page_html = get_page_url(705090)
# print(success_code)

page_no = 15
stories_link = []
# Scraping the links of the stories from the website.
# It's an infinite loop.
while(1):
  success_code, page_html = get_page_url(page_no)
  if success_code != 200:
    break
  get_stories_link(page_html)
  page_no += 15
  if(len(stories_link) > 8000):
    break
  # wait_time = random.uniform(1,4)
  # time.sleep(1)

# stories_link[-1]

# stories_link = stories_link[0:-50]

# It's scraping the stories and the authors from the links.
story_texts = []
authors = []
for link in stories_link:
    # print(link)
    get_html = requests.get(link).text
    soup = BeautifulSoup(get_html, 'lxml')
    # print('\n');
    try:
      story = soup.find('div', class_ = 'blog-content').text.strip()
      story_texts.append(story)
    except:
      # authors.append(np.nan)
      story_texts.append(np.nan)
    try:
      writer = soup.find('span', class_= 'word_break').text
      authors.append(writer)
    except:
      authors.append(np.nan)

    # print(writer)
    # break


# It's creating a dataframe from the lists of authors and story texts.
df = pd.DataFrame({'Author' : authors, 'Texts' : story_texts})

df_drop_na = df.dropna()

df_drop_na.info()

df_drop_na.to_csv('Somewhereinblogs.csv', index = False)

